{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from rl.agent import DQNAgent\n",
    "from gymjsp.jsspenv import HeuristicJsspEnv\n",
    "from tianshou_ppo import tianshou_ppo_train\n",
    "from ortools_scheduler import ORtools_scheduler\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\"ft06\", \"la01\", \"la06\", \"la11\", \"la21\", \"la31\", \"la36\", \"orb01\", \"swv01\", \"swv06\", \"swv11\", \"yn1\"]\n",
    "\n",
    "num_episodes = 1000\n",
    "memory_size = 100000\n",
    "batch_size = 64\n",
    "target_update = 100\n",
    "noisy = False\n",
    "plotting_inteval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rate = 0.5\n",
    "cv = 0.2\n",
    "n = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'policies/dqn_mlp/swv11_num_episodes=1000_memory_size=100000_target_update=100_noisy=False.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m env \u001b[38;5;241m=\u001b[39m HeuristicJsspEnv(instance)\n\u001b[1;32m     18\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, memory_size, batch_size, target_update, noisy\u001b[38;5;241m=\u001b[39mnoisy)\n\u001b[0;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m makespan \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtest()\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39m_get_dqn()\n",
      "File \u001b[0;32m~/code/DRL-for-Job-Shop-Scheduling/rl/agent.py:268\u001b[0m, in \u001b[0;36mDQNAgent.load_dqn\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dqn\u001b[39m(\u001b[38;5;28mself\u001b[39m, file):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'policies/dqn_mlp/swv11_num_episodes=1000_memory_size=100000_target_update=100_noisy=False.pth'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory = f\"figs_no_future_infomation/p{random_rate}cv{cv}num_episodes{num_episodes}_dqn\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "ret = {}\n",
    "\n",
    "ortools_mean, ortools_std, policy_mean, policy_std = [], [], [], []\n",
    "ortools_on_original, policy_on_original = [], []\n",
    "optimal_mean, optimal_std = [], []\n",
    "ortools_300s_optimal_rate = []\n",
    "current_instances = []\n",
    "\n",
    "for instance in instances:\n",
    "    sols_directory = f\"sols/{instance}/p{random_rate}cv{cv}\"\n",
    "    policy_file = f\"policies/dqn_mlp/{instance}_num_episodes={num_episodes}_memory_size={memory_size}_target_update={target_update}_noisy={noisy}.pth\"\n",
    "    env = HeuristicJsspEnv(instance)\n",
    "    agent = DQNAgent(env, memory_size, batch_size, target_update, noisy=noisy)\n",
    "    agent.load_dqn(policy_file)\n",
    "    makespan = agent.test()\n",
    "    model = agent._get_dqn()\n",
    "    #print(next(model.parameters()).device)\n",
    "\n",
    "    #makespan, policy = tianshou_ppo_train(instance_name=instance, max_epoch=epochs)\n",
    "    ret[instance] = makespan\n",
    "\n",
    "    scheduler = ORtools_scheduler(instance)\n",
    "    #scheduler.optimize()\n",
    "    #obj_val = scheduler.obj_val\n",
    "    scheduler.read_solution()           # 读取静态解\n",
    "    obj_val = scheduler.compute_makespan()\n",
    "\n",
    "    policy_vals, ortools_vals, optimal_vals, if_optimals = [], [], [], []\n",
    "\n",
    "    for i in range(n):\n",
    "        #times = scheduler.shifted_time_(random_rate=random_rate, cv=cv)\n",
    "        #policy_val = scheduler.policy_makespan('ppo', policy, shifted_time=times)\n",
    "\n",
    "        scheduler.load_time_mat(os.path.join(sols_directory, f\"{i}.npy\"))\n",
    "        times = scheduler.times\n",
    "        \n",
    "        policy_val = scheduler.policy_makespan('dqn', model, shifted_time=times)\n",
    "        ortools_val = scheduler.compute_makespan(shifted_time=times)        # 静态调度面对工时波动\n",
    "\n",
    "        #if_optimal, optimal_val = scheduler.get_optimal_of_new_time_mat(times)\n",
    "\n",
    "        policy_vals.append(policy_val)\n",
    "        ortools_vals.append(ortools_val)\n",
    "        #optimal_vals.append(optimal_val)\n",
    "        #if_optimals.append(int(if_optimal))\n",
    "\n",
    "    info_df = pd.read_csv(os.path.join(sols_directory, \"info.csv\")) ##############################\n",
    "    optimal_vals, if_optimals = info_df['obj_val'].values.tolist(), info_df['optimal'].values.tolist()\n",
    "\n",
    "    plt.plot(policy_vals, color='g', label='policy')\n",
    "    plt.plot(ortools_vals, color='r', label='ortools_static')\n",
    "    plt.plot(optimal_vals, color='blue', label='ortools_300s')\n",
    "    policy_vals, ortools_vals, optimal_vals = np.array(policy_vals), np.array(ortools_vals), np.array(optimal_vals)\n",
    "\n",
    "    plt.hlines(np.mean(ortools_vals), -2, n+2, linestyles='dotted', colors='r')\n",
    "    plt.hlines(np.mean(policy_vals), -2, n+2, linestyles='dotted', colors='g')\n",
    "    plt.hlines(np.mean(optimal_vals), -2, n+2, linestyles='dotted', colors='blue')\n",
    "    scatter_x = np.where(if_optimals)\n",
    "    scatter_y = np.array(optimal_vals)[scatter_x]\n",
    "    plt.scatter(scatter_x, scatter_y, color='blue')\n",
    "\n",
    "\n",
    "\n",
    "    ortools_mean.append(np.mean(ortools_vals))\n",
    "    ortools_std.append(np.std(ortools_vals))\n",
    "    policy_mean.append(np.mean(policy_vals))\n",
    "    policy_std.append(np.std(policy_vals))\n",
    "    optimal_mean.append(np.mean(optimal_vals))\n",
    "    optimal_std.append(np.std(optimal_vals))\n",
    "    ortools_on_original.append(obj_val)\n",
    "    policy_on_original.append(makespan)\n",
    "    ortools_300s_optimal_rate.append(np.mean(if_optimals))\n",
    "    current_instances.append(instance)\n",
    "\n",
    "    plt.xlabel('trial')\n",
    "    plt.ylabel('makespan')\n",
    "    plt.title(f\"random_rate={random_rate},cv={cv},instance={instance}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{directory}/policy_vs_ortools_{instance}.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "# 将每个列添加到 DataFrame 中\n",
    "df['instance'] = current_instances\n",
    "df['ortools_mean'] = ortools_mean\n",
    "df['policy_mean'] = policy_mean\n",
    "df['optimal_mean'] = optimal_mean\n",
    "df['ortools_std'] = ortools_std\n",
    "df['policy_std'] = policy_std\n",
    "df['optimal_std'] = optimal_std\n",
    "df['ortools_on_original'] = ortools_on_original\n",
    "df['policy_on_original'] = policy_on_original\n",
    "df['ortools_300s_optimal_rate'] = ortools_300s_optimal_rate\n",
    "\n",
    "if os.path.exists(f\"{directory}/data.csv\"):\n",
    "    df2 = pd.read_csv(f\"{directory}/data.csv\")\n",
    "    df2 = df2.append(df, ignore_index=True)\n",
    "    df2.to_csv(f\"{directory}/data.csv\", index=False)\n",
    "else:\n",
    "    df.to_csv(f\"{directory}/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft06': 65,\n",
       " 'la01': 675,\n",
       " 'la06': 967,\n",
       " 'la11': 1259,\n",
       " 'la21': 1234,\n",
       " 'la31': 1819,\n",
       " 'la36': 1453,\n",
       " 'orb01': 1139,\n",
       " 'swv01': 1635,\n",
       " 'swv06': 2077}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dce9ce0b8994960befdaf1abc06919148bccb19973be1d090d69590bd56698c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
